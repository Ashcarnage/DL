{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionModel:\n",
    "    def __init__(self,x,y):\n",
    "        self.w = torch.rand([1],requires_grad=True)\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.gradients  = {i:[] for i in [\"w\",\"b\"]}\n",
    "        self.b = torch.rand([1],requires_grad=True)\n",
    "    def forward(self,x):\n",
    "        return self.w*x + self.b\n",
    "    def update(self,lr):\n",
    "        self.w -= lr*self.w.grad\n",
    "        self.b -= lr*self.b.grad\n",
    "        self.gradients[\"w\"].append(self.w.grad.clone())\n",
    "        self.gradients['b'].append(self.b.bgrad.clone())\n",
    "    def reset_grad(self):\n",
    "        self.w.grad.zero_()\n",
    "        self.b.grad.zero_()\n",
    "    def train(self,lr,epochs):\n",
    "        # Batch training\n",
    "        total_loss = []\n",
    "        for epochs in range(epochs):\n",
    "            loss = 0\n",
    "            for j in range(len(self.x)):\n",
    "                predicted = self.forward(self.x[j])\n",
    "                loss+=(self.y[j]-predicted)**2\n",
    "            loss = loss/len(self.x)\n",
    "            total_loss.append(loss.item())\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                self.update(lr)\n",
    "            self.reset_grad()\n",
    "            # print(f\"LOSS : {loss}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS : tensor([5.7914], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([5.0938], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([4.4803], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([3.9410], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([3.4667], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([3.0498], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([2.6832], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([2.3608], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([2.0774], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([1.8282], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([1.6091], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([1.4164], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([1.2471], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([1.0981], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.9672], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.8520], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.7508], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.6618], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.5835], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.5147], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.4542], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.4010], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.3543], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.3131], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.2770], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.2452], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.2172], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.1927], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.1710], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.1520], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.1353], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.1207], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.1077], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0964], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0864], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0776], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0699], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0631], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0571], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0519], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0473], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0432], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0397], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0365], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0338], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0313], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0292], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0273], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0257], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0242], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0230], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0219], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0209], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0200], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0192], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0186], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0180], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0175], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0170], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0166], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0163], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0159], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0157], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0154], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0152], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0150], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0149], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0147], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0146], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0145], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0144], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0143], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0142], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0142], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0141], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0141], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0140], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0140], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0139], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0139], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0139], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0139], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0138], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0138], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0138], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0138], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0138], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0138], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0138], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0138], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "LOSS : tensor([0.0137], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor( [12.4, 14.3, 14.5, 14.9, 16.1, 16.9, 16.5, 15.4, 17.0, 17.9, 18.8, 20.3, 22.4,\n",
    "19.4, 15.5, 16.7, 17.3, 18.4, 19.2, 17.4, 19.5, 19.7, 21.2])\n",
    "y = torch.tensor( [11.2, 12.5, 12.7, 13.1, 14.1, 14.8, 14.4, 13.4, 14.9, 15.6, 16.4, 17.7, 19.6,\n",
    "16.9, 14.0, 14.6, 15.1, 16.1, 16.8, 15.2, 17.0, 17.2, 18.6])\n",
    "model = RegressionModel(x,y)\n",
    "model.train(0.0001,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([-190.1251]), tensor([-189.7107])]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([2,4])\n",
    "y = torch.tensor([20,40])\n",
    "\n",
    "model = RegressionModel(x,y)\n",
    "model.train(0.0001,200)\n",
    "model.gradients['w'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionModel:\n",
    "    def __init__(self, x, y):\n",
    "        self.w = torch.rand([1],requires_grad=True)\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.gradients  = {i:[] for i in [\"w\",\"b\"]}\n",
    "        self.b = torch.rand([1],requires_grad=True)\n",
    "    def forward(self, x):\n",
    "        return x * self.w + self.b\n",
    "    def update(self,lr):\n",
    "        self.w -= lr*self.w.grad\n",
    "        self.b -= lr*self.b.grad\n",
    "        self.gradients['w'].append(self.w.grad.clone())\n",
    "        self.gradients['b'].append(self.b.grad.clone())\n",
    "    def train(self, lr, epochs):\n",
    "        self.total_loss = []\n",
    "        for i in range(epochs):\n",
    "            loss = 0\n",
    "            for j in range(len(self.x)):\n",
    "                y_pred = self.forward(self.x[j])\n",
    "                loss += (self.y[j]-y_pred)**2\n",
    "            loss = loss/len(self.x)\n",
    "            self.total_loss.append(loss.detach().numpy().item())\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                self.update(lr)\n",
    "            self.w.grad.zero_()\n",
    "            self.b.grad.zero_()\n",
    "            print(f\"epoch {i+1}  LOSS : {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1  LOSS : 25891.20703125\n",
      "epoch 2  LOSS : 11177.158203125\n",
      "epoch 3  LOSS : 4984.3369140625\n",
      "epoch 4  LOSS : 2377.911376953125\n",
      "epoch 5  LOSS : 1280.91845703125\n",
      "epoch 6  LOSS : 819.2110595703125\n",
      "epoch 7  LOSS : 624.8818359375\n",
      "epoch 8  LOSS : 543.0858154296875\n",
      "epoch 9  LOSS : 508.652587890625\n",
      "epoch 10  LOSS : 494.15289306640625\n",
      "epoch 11  LOSS : 488.04327392578125\n",
      "epoch 12  LOSS : 485.4647521972656\n",
      "epoch 13  LOSS : 484.372314453125\n",
      "epoch 14  LOSS : 483.90533447265625\n",
      "epoch 15  LOSS : 483.7017517089844\n",
      "epoch 16  LOSS : 483.60882568359375\n",
      "epoch 17  LOSS : 483.56268310546875\n",
      "epoch 18  LOSS : 483.53594970703125\n",
      "epoch 19  LOSS : 483.517333984375\n",
      "epoch 20  LOSS : 483.50250244140625\n",
      "epoch 21  LOSS : 483.48895263671875\n",
      "epoch 22  LOSS : 483.47625732421875\n",
      "epoch 23  LOSS : 483.463623046875\n",
      "epoch 24  LOSS : 483.4513244628906\n",
      "epoch 25  LOSS : 483.4388732910156\n",
      "epoch 26  LOSS : 483.4266052246094\n",
      "epoch 27  LOSS : 483.4141540527344\n",
      "epoch 28  LOSS : 483.4019470214844\n",
      "epoch 29  LOSS : 483.3895568847656\n",
      "epoch 30  LOSS : 483.3773498535156\n",
      "epoch 31  LOSS : 483.36505126953125\n",
      "epoch 32  LOSS : 483.3526306152344\n",
      "epoch 33  LOSS : 483.34063720703125\n",
      "epoch 34  LOSS : 483.3284606933594\n",
      "epoch 35  LOSS : 483.31610107421875\n",
      "epoch 36  LOSS : 483.3038024902344\n",
      "epoch 37  LOSS : 483.29150390625\n",
      "epoch 38  LOSS : 483.2796325683594\n",
      "epoch 39  LOSS : 483.26715087890625\n",
      "epoch 40  LOSS : 483.2549743652344\n",
      "epoch 41  LOSS : 483.24298095703125\n",
      "epoch 42  LOSS : 483.23046875\n",
      "epoch 43  LOSS : 483.21826171875\n",
      "epoch 44  LOSS : 483.20623779296875\n",
      "epoch 45  LOSS : 483.1940002441406\n",
      "epoch 46  LOSS : 483.18182373046875\n",
      "epoch 47  LOSS : 483.1698303222656\n",
      "epoch 48  LOSS : 483.157470703125\n",
      "epoch 49  LOSS : 483.1453552246094\n",
      "epoch 50  LOSS : 483.1333923339844\n",
      "epoch 51  LOSS : 483.121337890625\n",
      "epoch 52  LOSS : 483.10906982421875\n",
      "epoch 53  LOSS : 483.09716796875\n",
      "epoch 54  LOSS : 483.0850524902344\n",
      "epoch 55  LOSS : 483.07275390625\n",
      "epoch 56  LOSS : 483.0608825683594\n",
      "epoch 57  LOSS : 483.04876708984375\n",
      "epoch 58  LOSS : 483.0367736816406\n",
      "epoch 59  LOSS : 483.0245056152344\n",
      "epoch 60  LOSS : 483.01300048828125\n",
      "epoch 61  LOSS : 483.00042724609375\n",
      "epoch 62  LOSS : 482.98858642578125\n",
      "epoch 63  LOSS : 482.97662353515625\n",
      "epoch 64  LOSS : 482.9646911621094\n",
      "epoch 65  LOSS : 482.95269775390625\n",
      "epoch 66  LOSS : 482.94049072265625\n",
      "epoch 67  LOSS : 482.92877197265625\n",
      "epoch 68  LOSS : 482.9165954589844\n",
      "epoch 69  LOSS : 482.9048767089844\n",
      "epoch 70  LOSS : 482.89276123046875\n",
      "epoch 71  LOSS : 482.8812561035156\n",
      "epoch 72  LOSS : 482.86883544921875\n",
      "epoch 73  LOSS : 482.8570251464844\n",
      "epoch 74  LOSS : 482.84521484375\n",
      "epoch 75  LOSS : 482.833251953125\n",
      "epoch 76  LOSS : 482.82135009765625\n",
      "epoch 77  LOSS : 482.80938720703125\n",
      "epoch 78  LOSS : 482.797607421875\n",
      "epoch 79  LOSS : 482.785888671875\n",
      "epoch 80  LOSS : 482.77349853515625\n",
      "epoch 81  LOSS : 482.761962890625\n",
      "epoch 82  LOSS : 482.75018310546875\n",
      "epoch 83  LOSS : 482.7384338378906\n",
      "epoch 84  LOSS : 482.72625732421875\n",
      "epoch 85  LOSS : 482.71466064453125\n",
      "epoch 86  LOSS : 482.7027893066406\n",
      "epoch 87  LOSS : 482.69097900390625\n",
      "epoch 88  LOSS : 482.67926025390625\n",
      "epoch 89  LOSS : 482.667236328125\n",
      "epoch 90  LOSS : 482.65545654296875\n",
      "epoch 91  LOSS : 482.6441345214844\n",
      "epoch 92  LOSS : 482.6321716308594\n",
      "epoch 93  LOSS : 482.62042236328125\n",
      "epoch 94  LOSS : 482.60845947265625\n",
      "epoch 95  LOSS : 482.5970764160156\n",
      "epoch 96  LOSS : 482.5852966308594\n",
      "epoch 97  LOSS : 482.5733947753906\n",
      "epoch 98  LOSS : 482.5616760253906\n",
      "epoch 99  LOSS : 482.5498962402344\n",
      "epoch 100  LOSS : 482.53839111328125\n",
      "epoch 101  LOSS : 482.526611328125\n",
      "epoch 102  LOSS : 482.51470947265625\n",
      "epoch 103  LOSS : 482.5035705566406\n",
      "epoch 104  LOSS : 482.4916076660156\n",
      "epoch 105  LOSS : 482.47998046875\n",
      "epoch 106  LOSS : 482.46856689453125\n",
      "epoch 107  LOSS : 482.456787109375\n",
      "epoch 108  LOSS : 482.4451599121094\n",
      "epoch 109  LOSS : 482.43359375\n",
      "epoch 110  LOSS : 482.4215393066406\n",
      "epoch 111  LOSS : 482.40997314453125\n",
      "epoch 112  LOSS : 482.3985290527344\n",
      "epoch 113  LOSS : 482.38714599609375\n",
      "epoch 114  LOSS : 482.3750915527344\n",
      "epoch 115  LOSS : 482.36370849609375\n",
      "epoch 116  LOSS : 482.35211181640625\n",
      "epoch 117  LOSS : 482.3404846191406\n",
      "epoch 118  LOSS : 482.3292541503906\n",
      "epoch 119  LOSS : 482.31787109375\n",
      "epoch 120  LOSS : 482.30596923828125\n",
      "epoch 121  LOSS : 482.2948303222656\n",
      "epoch 122  LOSS : 482.28289794921875\n",
      "epoch 123  LOSS : 482.27178955078125\n",
      "epoch 124  LOSS : 482.2596740722656\n",
      "epoch 125  LOSS : 482.24847412109375\n",
      "epoch 126  LOSS : 482.23681640625\n",
      "epoch 127  LOSS : 482.22540283203125\n",
      "epoch 128  LOSS : 482.2142028808594\n",
      "epoch 129  LOSS : 482.20245361328125\n",
      "epoch 130  LOSS : 482.1910705566406\n",
      "epoch 131  LOSS : 482.17962646484375\n",
      "epoch 132  LOSS : 482.1681213378906\n",
      "epoch 133  LOSS : 482.1571350097656\n",
      "epoch 134  LOSS : 482.14544677734375\n",
      "epoch 135  LOSS : 482.13427734375\n",
      "epoch 136  LOSS : 482.12255859375\n",
      "epoch 137  LOSS : 482.11102294921875\n",
      "epoch 138  LOSS : 482.0997009277344\n",
      "epoch 139  LOSS : 482.08819580078125\n",
      "epoch 140  LOSS : 482.07684326171875\n",
      "epoch 141  LOSS : 482.06591796875\n",
      "epoch 142  LOSS : 482.0542907714844\n",
      "epoch 143  LOSS : 482.04296875\n",
      "epoch 144  LOSS : 482.031494140625\n",
      "epoch 145  LOSS : 482.02020263671875\n",
      "epoch 146  LOSS : 482.00897216796875\n",
      "epoch 147  LOSS : 481.9976501464844\n",
      "epoch 148  LOSS : 481.9862365722656\n",
      "epoch 149  LOSS : 481.97515869140625\n",
      "epoch 150  LOSS : 481.9634704589844\n",
      "epoch 151  LOSS : 481.952392578125\n",
      "epoch 152  LOSS : 481.9413146972656\n",
      "epoch 153  LOSS : 481.9296875\n",
      "epoch 154  LOSS : 481.91876220703125\n",
      "epoch 155  LOSS : 481.90740966796875\n",
      "epoch 156  LOSS : 481.8961486816406\n",
      "epoch 157  LOSS : 481.8848571777344\n",
      "epoch 158  LOSS : 481.87359619140625\n",
      "epoch 159  LOSS : 481.8624572753906\n",
      "epoch 160  LOSS : 481.85125732421875\n",
      "epoch 161  LOSS : 481.8399353027344\n",
      "epoch 162  LOSS : 481.82855224609375\n",
      "epoch 163  LOSS : 481.8174743652344\n",
      "epoch 164  LOSS : 481.80657958984375\n",
      "epoch 165  LOSS : 481.79522705078125\n",
      "epoch 166  LOSS : 481.78436279296875\n",
      "epoch 167  LOSS : 481.7730407714844\n",
      "epoch 168  LOSS : 481.76190185546875\n",
      "epoch 169  LOSS : 481.7505798339844\n",
      "epoch 170  LOSS : 481.73944091796875\n",
      "epoch 171  LOSS : 481.7283630371094\n",
      "epoch 172  LOSS : 481.71746826171875\n",
      "epoch 173  LOSS : 481.70611572265625\n",
      "epoch 174  LOSS : 481.6949768066406\n",
      "epoch 175  LOSS : 481.68365478515625\n",
      "epoch 176  LOSS : 481.672607421875\n",
      "epoch 177  LOSS : 481.6617126464844\n",
      "epoch 178  LOSS : 481.65069580078125\n",
      "epoch 179  LOSS : 481.6394958496094\n",
      "epoch 180  LOSS : 481.62841796875\n",
      "epoch 181  LOSS : 481.61761474609375\n",
      "epoch 182  LOSS : 481.6063537597656\n",
      "epoch 183  LOSS : 481.59539794921875\n",
      "epoch 184  LOSS : 481.58428955078125\n",
      "epoch 185  LOSS : 481.57318115234375\n",
      "epoch 186  LOSS : 481.5621032714844\n",
      "epoch 187  LOSS : 481.5511779785156\n",
      "epoch 188  LOSS : 481.54022216796875\n",
      "epoch 189  LOSS : 481.52911376953125\n",
      "epoch 190  LOSS : 481.51849365234375\n",
      "epoch 191  LOSS : 481.50714111328125\n",
      "epoch 192  LOSS : 481.49652099609375\n",
      "epoch 193  LOSS : 481.4854431152344\n",
      "epoch 194  LOSS : 481.474609375\n",
      "epoch 195  LOSS : 481.4632873535156\n",
      "epoch 196  LOSS : 481.45233154296875\n",
      "epoch 197  LOSS : 481.4418029785156\n",
      "epoch 198  LOSS : 481.4308166503906\n",
      "epoch 199  LOSS : 481.41986083984375\n",
      "epoch 200  LOSS : 481.4090270996094\n",
      "epoch 201  LOSS : 481.3980407714844\n",
      "epoch 202  LOSS : 481.3871154785156\n",
      "epoch 203  LOSS : 481.3765563964844\n",
      "epoch 204  LOSS : 481.36529541015625\n",
      "epoch 205  LOSS : 481.35455322265625\n",
      "epoch 206  LOSS : 481.34356689453125\n",
      "epoch 207  LOSS : 481.332763671875\n",
      "epoch 208  LOSS : 481.32171630859375\n",
      "epoch 209  LOSS : 481.3111267089844\n",
      "epoch 210  LOSS : 481.30059814453125\n",
      "epoch 211  LOSS : 481.2893981933594\n",
      "epoch 212  LOSS : 481.27880859375\n",
      "epoch 213  LOSS : 481.26776123046875\n",
      "epoch 214  LOSS : 481.2572326660156\n",
      "epoch 215  LOSS : 481.2461853027344\n",
      "epoch 216  LOSS : 481.2354431152344\n",
      "epoch 217  LOSS : 481.2247009277344\n",
      "epoch 218  LOSS : 481.21392822265625\n",
      "epoch 219  LOSS : 481.20306396484375\n",
      "epoch 220  LOSS : 481.1922912597656\n",
      "epoch 221  LOSS : 481.18170166015625\n",
      "epoch 222  LOSS : 481.17108154296875\n",
      "epoch 223  LOSS : 481.16015625\n",
      "epoch 224  LOSS : 481.14935302734375\n",
      "epoch 225  LOSS : 481.1385192871094\n",
      "epoch 226  LOSS : 481.1279296875\n",
      "epoch 227  LOSS : 481.1170959472656\n",
      "epoch 228  LOSS : 481.10662841796875\n",
      "epoch 229  LOSS : 481.0960388183594\n",
      "epoch 230  LOSS : 481.085205078125\n",
      "epoch 231  LOSS : 481.07440185546875\n",
      "epoch 232  LOSS : 481.0641174316406\n",
      "epoch 233  LOSS : 481.0533142089844\n",
      "epoch 234  LOSS : 481.04290771484375\n",
      "epoch 235  LOSS : 481.03216552734375\n",
      "epoch 236  LOSS : 481.0210876464844\n",
      "epoch 237  LOSS : 481.01055908203125\n",
      "epoch 238  LOSS : 481.00030517578125\n",
      "epoch 239  LOSS : 480.9893493652344\n",
      "epoch 240  LOSS : 480.9789123535156\n",
      "epoch 241  LOSS : 480.96807861328125\n",
      "epoch 242  LOSS : 480.957763671875\n",
      "epoch 243  LOSS : 480.9471740722656\n",
      "epoch 244  LOSS : 480.93646240234375\n",
      "epoch 245  LOSS : 480.926025390625\n",
      "epoch 246  LOSS : 480.91546630859375\n",
      "epoch 247  LOSS : 480.9046936035156\n",
      "epoch 248  LOSS : 480.8943786621094\n",
      "epoch 249  LOSS : 480.8833923339844\n",
      "epoch 250  LOSS : 480.8731994628906\n",
      "epoch 251  LOSS : 480.86285400390625\n",
      "epoch 252  LOSS : 480.85247802734375\n",
      "epoch 253  LOSS : 480.84173583984375\n",
      "epoch 254  LOSS : 480.83087158203125\n",
      "epoch 255  LOSS : 480.82061767578125\n",
      "epoch 256  LOSS : 480.81005859375\n",
      "epoch 257  LOSS : 480.799560546875\n",
      "epoch 258  LOSS : 480.78936767578125\n",
      "epoch 259  LOSS : 480.7789001464844\n",
      "epoch 260  LOSS : 480.76824951171875\n",
      "epoch 261  LOSS : 480.75787353515625\n",
      "epoch 262  LOSS : 480.74755859375\n",
      "epoch 263  LOSS : 480.73681640625\n",
      "epoch 264  LOSS : 480.72650146484375\n",
      "epoch 265  LOSS : 480.71600341796875\n",
      "epoch 266  LOSS : 480.7056579589844\n",
      "epoch 267  LOSS : 480.69512939453125\n",
      "epoch 268  LOSS : 480.68487548828125\n",
      "epoch 269  LOSS : 480.67449951171875\n",
      "epoch 270  LOSS : 480.66424560546875\n",
      "epoch 271  LOSS : 480.65380859375\n",
      "epoch 272  LOSS : 480.6434631347656\n",
      "epoch 273  LOSS : 480.63299560546875\n",
      "epoch 274  LOSS : 480.6226501464844\n",
      "epoch 275  LOSS : 480.612548828125\n",
      "epoch 276  LOSS : 480.60186767578125\n",
      "epoch 277  LOSS : 480.5916442871094\n",
      "epoch 278  LOSS : 480.58148193359375\n",
      "epoch 279  LOSS : 480.5709533691406\n",
      "epoch 280  LOSS : 480.56072998046875\n",
      "epoch 281  LOSS : 480.55047607421875\n",
      "epoch 282  LOSS : 480.5399475097656\n",
      "epoch 283  LOSS : 480.530029296875\n",
      "epoch 284  LOSS : 480.5196228027344\n",
      "epoch 285  LOSS : 480.50933837890625\n",
      "epoch 286  LOSS : 480.4990234375\n",
      "epoch 287  LOSS : 480.48858642578125\n",
      "epoch 288  LOSS : 480.4784240722656\n",
      "epoch 289  LOSS : 480.46844482421875\n",
      "epoch 290  LOSS : 480.4580078125\n",
      "epoch 291  LOSS : 480.447509765625\n",
      "epoch 292  LOSS : 480.43780517578125\n",
      "epoch 293  LOSS : 480.4273986816406\n",
      "epoch 294  LOSS : 480.4171447753906\n",
      "epoch 295  LOSS : 480.4066467285156\n",
      "epoch 296  LOSS : 480.3968200683594\n",
      "epoch 297  LOSS : 480.38653564453125\n",
      "epoch 298  LOSS : 480.3760681152344\n",
      "epoch 299  LOSS : 480.36627197265625\n",
      "epoch 300  LOSS : 480.35601806640625\n",
      "epoch 301  LOSS : 480.345947265625\n",
      "epoch 302  LOSS : 480.3357849121094\n",
      "epoch 303  LOSS : 480.3255920410156\n",
      "epoch 304  LOSS : 480.3157653808594\n",
      "epoch 305  LOSS : 480.30535888671875\n",
      "epoch 306  LOSS : 480.29510498046875\n",
      "epoch 307  LOSS : 480.28509521484375\n",
      "epoch 308  LOSS : 480.2750549316406\n",
      "epoch 309  LOSS : 480.2645568847656\n",
      "epoch 310  LOSS : 480.254638671875\n",
      "epoch 311  LOSS : 480.2447204589844\n",
      "epoch 312  LOSS : 480.2345275878906\n",
      "epoch 313  LOSS : 480.22454833984375\n",
      "epoch 314  LOSS : 480.2145080566406\n",
      "epoch 315  LOSS : 480.2044372558594\n",
      "epoch 316  LOSS : 480.1942443847656\n",
      "epoch 317  LOSS : 480.1844787597656\n",
      "epoch 318  LOSS : 480.1741638183594\n",
      "epoch 319  LOSS : 480.16436767578125\n",
      "epoch 320  LOSS : 480.1541442871094\n",
      "epoch 321  LOSS : 480.14410400390625\n",
      "epoch 322  LOSS : 480.13427734375\n",
      "epoch 323  LOSS : 480.12420654296875\n",
      "epoch 324  LOSS : 480.11395263671875\n",
      "epoch 325  LOSS : 480.1040954589844\n",
      "epoch 326  LOSS : 480.0940856933594\n",
      "epoch 327  LOSS : 480.08428955078125\n",
      "epoch 328  LOSS : 480.07421875\n",
      "epoch 329  LOSS : 480.0641174316406\n",
      "epoch 330  LOSS : 480.0541076660156\n",
      "epoch 331  LOSS : 480.044189453125\n",
      "epoch 332  LOSS : 480.03436279296875\n",
      "epoch 333  LOSS : 480.0245666503906\n",
      "epoch 334  LOSS : 480.0146484375\n",
      "epoch 335  LOSS : 480.00457763671875\n",
      "epoch 336  LOSS : 479.994873046875\n",
      "epoch 337  LOSS : 479.98486328125\n",
      "epoch 338  LOSS : 479.97454833984375\n",
      "epoch 339  LOSS : 479.96484375\n",
      "epoch 340  LOSS : 479.9551696777344\n",
      "epoch 341  LOSS : 479.94561767578125\n",
      "epoch 342  LOSS : 479.93548583984375\n",
      "epoch 343  LOSS : 479.9253845214844\n",
      "epoch 344  LOSS : 479.9156188964844\n",
      "epoch 345  LOSS : 479.90594482421875\n",
      "epoch 346  LOSS : 479.8959045410156\n",
      "epoch 347  LOSS : 479.88604736328125\n",
      "epoch 348  LOSS : 479.8761291503906\n",
      "epoch 349  LOSS : 479.866455078125\n",
      "epoch 350  LOSS : 479.85662841796875\n",
      "epoch 351  LOSS : 479.84686279296875\n",
      "epoch 352  LOSS : 479.837158203125\n",
      "epoch 353  LOSS : 479.82745361328125\n",
      "epoch 354  LOSS : 479.8173828125\n",
      "epoch 355  LOSS : 479.8076171875\n",
      "epoch 356  LOSS : 479.7979431152344\n",
      "epoch 357  LOSS : 479.7881774902344\n",
      "epoch 358  LOSS : 479.7784118652344\n",
      "epoch 359  LOSS : 479.7686462402344\n",
      "epoch 360  LOSS : 479.7586975097656\n",
      "epoch 361  LOSS : 479.7489318847656\n",
      "epoch 362  LOSS : 479.7391662597656\n",
      "epoch 363  LOSS : 479.7294921875\n",
      "epoch 364  LOSS : 479.71990966796875\n",
      "epoch 365  LOSS : 479.710205078125\n",
      "epoch 366  LOSS : 479.70050048828125\n",
      "epoch 367  LOSS : 479.6907653808594\n",
      "epoch 368  LOSS : 479.6812438964844\n",
      "epoch 369  LOSS : 479.6712341308594\n",
      "epoch 370  LOSS : 479.6617126464844\n",
      "epoch 371  LOSS : 479.65216064453125\n",
      "epoch 372  LOSS : 479.642333984375\n",
      "epoch 373  LOSS : 479.6328125\n",
      "epoch 374  LOSS : 479.62335205078125\n",
      "epoch 375  LOSS : 479.61358642578125\n",
      "epoch 376  LOSS : 479.60369873046875\n",
      "epoch 377  LOSS : 479.59423828125\n",
      "epoch 378  LOSS : 479.5848693847656\n",
      "epoch 379  LOSS : 479.57489013671875\n",
      "epoch 380  LOSS : 479.5654296875\n",
      "epoch 381  LOSS : 479.5557556152344\n",
      "epoch 382  LOSS : 479.5462341308594\n",
      "epoch 383  LOSS : 479.53643798828125\n",
      "epoch 384  LOSS : 479.52667236328125\n",
      "epoch 385  LOSS : 479.5174865722656\n",
      "epoch 386  LOSS : 479.50787353515625\n",
      "epoch 387  LOSS : 479.498291015625\n",
      "epoch 388  LOSS : 479.48846435546875\n",
      "epoch 389  LOSS : 479.4793395996094\n",
      "epoch 390  LOSS : 479.46978759765625\n",
      "epoch 391  LOSS : 479.4600524902344\n",
      "epoch 392  LOSS : 479.4503479003906\n",
      "epoch 393  LOSS : 479.4407653808594\n",
      "epoch 394  LOSS : 479.4317932128906\n",
      "epoch 395  LOSS : 479.421875\n",
      "epoch 396  LOSS : 479.412353515625\n",
      "epoch 397  LOSS : 479.4029235839844\n",
      "epoch 398  LOSS : 479.39361572265625\n",
      "epoch 399  LOSS : 479.3836975097656\n",
      "epoch 400  LOSS : 479.37451171875\n",
      "epoch 401  LOSS : 479.36517333984375\n",
      "epoch 402  LOSS : 479.35546875\n",
      "epoch 403  LOSS : 479.3462829589844\n",
      "epoch 404  LOSS : 479.33660888671875\n",
      "epoch 405  LOSS : 479.3270568847656\n",
      "epoch 406  LOSS : 479.3175354003906\n",
      "epoch 407  LOSS : 479.30828857421875\n",
      "epoch 408  LOSS : 479.2989196777344\n",
      "epoch 409  LOSS : 479.28936767578125\n",
      "epoch 410  LOSS : 479.280029296875\n",
      "epoch 411  LOSS : 479.2705993652344\n",
      "epoch 412  LOSS : 479.2613830566406\n",
      "epoch 413  LOSS : 479.25164794921875\n",
      "epoch 414  LOSS : 479.24267578125\n",
      "epoch 415  LOSS : 479.23345947265625\n",
      "epoch 416  LOSS : 479.22357177734375\n",
      "epoch 417  LOSS : 479.21453857421875\n",
      "epoch 418  LOSS : 479.204833984375\n",
      "epoch 419  LOSS : 479.19586181640625\n",
      "epoch 420  LOSS : 479.1861267089844\n",
      "epoch 421  LOSS : 479.17730712890625\n",
      "epoch 422  LOSS : 479.1676330566406\n",
      "epoch 423  LOSS : 479.15814208984375\n",
      "epoch 424  LOSS : 479.14923095703125\n",
      "epoch 425  LOSS : 479.1397399902344\n",
      "epoch 426  LOSS : 479.13031005859375\n",
      "epoch 427  LOSS : 479.12078857421875\n",
      "epoch 428  LOSS : 479.1117248535156\n",
      "epoch 429  LOSS : 479.102294921875\n",
      "epoch 430  LOSS : 479.09320068359375\n",
      "epoch 431  LOSS : 479.0841369628906\n",
      "epoch 432  LOSS : 479.0746154785156\n",
      "epoch 433  LOSS : 479.0654296875\n",
      "epoch 434  LOSS : 479.0559997558594\n",
      "epoch 435  LOSS : 479.04693603515625\n",
      "epoch 436  LOSS : 479.0376892089844\n",
      "epoch 437  LOSS : 479.02874755859375\n",
      "epoch 438  LOSS : 479.01922607421875\n",
      "epoch 439  LOSS : 479.0101013183594\n",
      "epoch 440  LOSS : 479.0006408691406\n",
      "epoch 441  LOSS : 478.99163818359375\n",
      "epoch 442  LOSS : 478.98236083984375\n",
      "epoch 443  LOSS : 478.9732360839844\n",
      "epoch 444  LOSS : 478.96392822265625\n",
      "epoch 445  LOSS : 478.95477294921875\n",
      "epoch 446  LOSS : 478.9457092285156\n",
      "epoch 447  LOSS : 478.9364318847656\n",
      "epoch 448  LOSS : 478.92742919921875\n",
      "epoch 449  LOSS : 478.9183044433594\n",
      "epoch 450  LOSS : 478.9091796875\n",
      "epoch 451  LOSS : 478.89984130859375\n",
      "epoch 452  LOSS : 478.8904724121094\n",
      "epoch 453  LOSS : 478.8816833496094\n",
      "epoch 454  LOSS : 478.87255859375\n",
      "epoch 455  LOSS : 478.863525390625\n",
      "epoch 456  LOSS : 478.85430908203125\n",
      "epoch 457  LOSS : 478.8451232910156\n",
      "epoch 458  LOSS : 478.8360290527344\n",
      "epoch 459  LOSS : 478.82684326171875\n",
      "epoch 460  LOSS : 478.8177795410156\n",
      "epoch 461  LOSS : 478.80865478515625\n",
      "epoch 462  LOSS : 478.7996520996094\n",
      "epoch 463  LOSS : 478.79071044921875\n",
      "epoch 464  LOSS : 478.78173828125\n",
      "epoch 465  LOSS : 478.77252197265625\n",
      "epoch 466  LOSS : 478.76336669921875\n",
      "epoch 467  LOSS : 478.754638671875\n",
      "epoch 468  LOSS : 478.74542236328125\n",
      "epoch 469  LOSS : 478.7364807128906\n",
      "epoch 470  LOSS : 478.727294921875\n",
      "epoch 471  LOSS : 478.718505859375\n",
      "epoch 472  LOSS : 478.70947265625\n",
      "epoch 473  LOSS : 478.70037841796875\n",
      "epoch 474  LOSS : 478.69134521484375\n",
      "epoch 475  LOSS : 478.68231201171875\n",
      "epoch 476  LOSS : 478.67315673828125\n",
      "epoch 477  LOSS : 478.66424560546875\n",
      "epoch 478  LOSS : 478.65521240234375\n",
      "epoch 479  LOSS : 478.64617919921875\n",
      "epoch 480  LOSS : 478.637451171875\n",
      "epoch 481  LOSS : 478.6283264160156\n",
      "epoch 482  LOSS : 478.61956787109375\n",
      "epoch 483  LOSS : 478.61065673828125\n",
      "epoch 484  LOSS : 478.60174560546875\n",
      "epoch 485  LOSS : 478.5926818847656\n",
      "epoch 486  LOSS : 478.583740234375\n",
      "epoch 487  LOSS : 478.57470703125\n",
      "epoch 488  LOSS : 478.56585693359375\n",
      "epoch 489  LOSS : 478.55682373046875\n",
      "epoch 490  LOSS : 478.54815673828125\n",
      "epoch 491  LOSS : 478.5392150878906\n",
      "epoch 492  LOSS : 478.5301208496094\n",
      "epoch 493  LOSS : 478.5216369628906\n",
      "epoch 494  LOSS : 478.51287841796875\n",
      "epoch 495  LOSS : 478.50372314453125\n",
      "epoch 496  LOSS : 478.4945373535156\n",
      "epoch 497  LOSS : 478.48602294921875\n",
      "epoch 498  LOSS : 478.47735595703125\n",
      "epoch 499  LOSS : 478.4683532714844\n",
      "epoch 500  LOSS : 478.4593811035156\n",
      "[[0, 25891.20703125], [1, 11177.158203125], [2, 4984.3369140625], [3, 2377.911376953125], [4, 1280.91845703125], [5, 819.2110595703125], [6, 624.8818359375], [7, 543.0858154296875], [8, 508.652587890625], [9, 494.15289306640625], [10, 488.04327392578125], [11, 485.4647521972656], [12, 484.372314453125], [13, 483.90533447265625], [14, 483.7017517089844], [15, 483.60882568359375], [16, 483.56268310546875], [17, 483.53594970703125], [18, 483.517333984375], [19, 483.50250244140625], [20, 483.48895263671875], [21, 483.47625732421875], [22, 483.463623046875], [23, 483.4513244628906], [24, 483.4388732910156], [25, 483.4266052246094], [26, 483.4141540527344], [27, 483.4019470214844], [28, 483.3895568847656], [29, 483.3773498535156], [30, 483.36505126953125], [31, 483.3526306152344], [32, 483.34063720703125], [33, 483.3284606933594], [34, 483.31610107421875], [35, 483.3038024902344], [36, 483.29150390625], [37, 483.2796325683594], [38, 483.26715087890625], [39, 483.2549743652344], [40, 483.24298095703125], [41, 483.23046875], [42, 483.21826171875], [43, 483.20623779296875], [44, 483.1940002441406], [45, 483.18182373046875], [46, 483.1698303222656], [47, 483.157470703125], [48, 483.1453552246094], [49, 483.1333923339844], [50, 483.121337890625], [51, 483.10906982421875], [52, 483.09716796875], [53, 483.0850524902344], [54, 483.07275390625], [55, 483.0608825683594], [56, 483.04876708984375], [57, 483.0367736816406], [58, 483.0245056152344], [59, 483.01300048828125], [60, 483.00042724609375], [61, 482.98858642578125], [62, 482.97662353515625], [63, 482.9646911621094], [64, 482.95269775390625], [65, 482.94049072265625], [66, 482.92877197265625], [67, 482.9165954589844], [68, 482.9048767089844], [69, 482.89276123046875], [70, 482.8812561035156], [71, 482.86883544921875], [72, 482.8570251464844], [73, 482.84521484375], [74, 482.833251953125], [75, 482.82135009765625], [76, 482.80938720703125], [77, 482.797607421875], [78, 482.785888671875], [79, 482.77349853515625], [80, 482.761962890625], [81, 482.75018310546875], [82, 482.7384338378906], [83, 482.72625732421875], [84, 482.71466064453125], [85, 482.7027893066406], [86, 482.69097900390625], [87, 482.67926025390625], [88, 482.667236328125], [89, 482.65545654296875], [90, 482.6441345214844], [91, 482.6321716308594], [92, 482.62042236328125], [93, 482.60845947265625], [94, 482.5970764160156], [95, 482.5852966308594], [96, 482.5733947753906], [97, 482.5616760253906], [98, 482.5498962402344], [99, 482.53839111328125], [100, 482.526611328125], [101, 482.51470947265625], [102, 482.5035705566406], [103, 482.4916076660156], [104, 482.47998046875], [105, 482.46856689453125], [106, 482.456787109375], [107, 482.4451599121094], [108, 482.43359375], [109, 482.4215393066406], [110, 482.40997314453125], [111, 482.3985290527344], [112, 482.38714599609375], [113, 482.3750915527344], [114, 482.36370849609375], [115, 482.35211181640625], [116, 482.3404846191406], [117, 482.3292541503906], [118, 482.31787109375], [119, 482.30596923828125], [120, 482.2948303222656], [121, 482.28289794921875], [122, 482.27178955078125], [123, 482.2596740722656], [124, 482.24847412109375], [125, 482.23681640625], [126, 482.22540283203125], [127, 482.2142028808594], [128, 482.20245361328125], [129, 482.1910705566406], [130, 482.17962646484375], [131, 482.1681213378906], [132, 482.1571350097656], [133, 482.14544677734375], [134, 482.13427734375], [135, 482.12255859375], [136, 482.11102294921875], [137, 482.0997009277344], [138, 482.08819580078125], [139, 482.07684326171875], [140, 482.06591796875], [141, 482.0542907714844], [142, 482.04296875], [143, 482.031494140625], [144, 482.02020263671875], [145, 482.00897216796875], [146, 481.9976501464844], [147, 481.9862365722656], [148, 481.97515869140625], [149, 481.9634704589844], [150, 481.952392578125], [151, 481.9413146972656], [152, 481.9296875], [153, 481.91876220703125], [154, 481.90740966796875], [155, 481.8961486816406], [156, 481.8848571777344], [157, 481.87359619140625], [158, 481.8624572753906], [159, 481.85125732421875], [160, 481.8399353027344], [161, 481.82855224609375], [162, 481.8174743652344], [163, 481.80657958984375], [164, 481.79522705078125], [165, 481.78436279296875], [166, 481.7730407714844], [167, 481.76190185546875], [168, 481.7505798339844], [169, 481.73944091796875], [170, 481.7283630371094], [171, 481.71746826171875], [172, 481.70611572265625], [173, 481.6949768066406], [174, 481.68365478515625], [175, 481.672607421875], [176, 481.6617126464844], [177, 481.65069580078125], [178, 481.6394958496094], [179, 481.62841796875], [180, 481.61761474609375], [181, 481.6063537597656], [182, 481.59539794921875], [183, 481.58428955078125], [184, 481.57318115234375], [185, 481.5621032714844], [186, 481.5511779785156], [187, 481.54022216796875], [188, 481.52911376953125], [189, 481.51849365234375], [190, 481.50714111328125], [191, 481.49652099609375], [192, 481.4854431152344], [193, 481.474609375], [194, 481.4632873535156], [195, 481.45233154296875], [196, 481.4418029785156], [197, 481.4308166503906], [198, 481.41986083984375], [199, 481.4090270996094], [200, 481.3980407714844], [201, 481.3871154785156], [202, 481.3765563964844], [203, 481.36529541015625], [204, 481.35455322265625], [205, 481.34356689453125], [206, 481.332763671875], [207, 481.32171630859375], [208, 481.3111267089844], [209, 481.30059814453125], [210, 481.2893981933594], [211, 481.27880859375], [212, 481.26776123046875], [213, 481.2572326660156], [214, 481.2461853027344], [215, 481.2354431152344], [216, 481.2247009277344], [217, 481.21392822265625], [218, 481.20306396484375], [219, 481.1922912597656], [220, 481.18170166015625], [221, 481.17108154296875], [222, 481.16015625], [223, 481.14935302734375], [224, 481.1385192871094], [225, 481.1279296875], [226, 481.1170959472656], [227, 481.10662841796875], [228, 481.0960388183594], [229, 481.085205078125], [230, 481.07440185546875], [231, 481.0641174316406], [232, 481.0533142089844], [233, 481.04290771484375], [234, 481.03216552734375], [235, 481.0210876464844], [236, 481.01055908203125], [237, 481.00030517578125], [238, 480.9893493652344], [239, 480.9789123535156], [240, 480.96807861328125], [241, 480.957763671875], [242, 480.9471740722656], [243, 480.93646240234375], [244, 480.926025390625], [245, 480.91546630859375], [246, 480.9046936035156], [247, 480.8943786621094], [248, 480.8833923339844], [249, 480.8731994628906], [250, 480.86285400390625], [251, 480.85247802734375], [252, 480.84173583984375], [253, 480.83087158203125], [254, 480.82061767578125], [255, 480.81005859375], [256, 480.799560546875], [257, 480.78936767578125], [258, 480.7789001464844], [259, 480.76824951171875], [260, 480.75787353515625], [261, 480.74755859375], [262, 480.73681640625], [263, 480.72650146484375], [264, 480.71600341796875], [265, 480.7056579589844], [266, 480.69512939453125], [267, 480.68487548828125], [268, 480.67449951171875], [269, 480.66424560546875], [270, 480.65380859375], [271, 480.6434631347656], [272, 480.63299560546875], [273, 480.6226501464844], [274, 480.612548828125], [275, 480.60186767578125], [276, 480.5916442871094], [277, 480.58148193359375], [278, 480.5709533691406], [279, 480.56072998046875], [280, 480.55047607421875], [281, 480.5399475097656], [282, 480.530029296875], [283, 480.5196228027344], [284, 480.50933837890625], [285, 480.4990234375], [286, 480.48858642578125], [287, 480.4784240722656], [288, 480.46844482421875], [289, 480.4580078125], [290, 480.447509765625], [291, 480.43780517578125], [292, 480.4273986816406], [293, 480.4171447753906], [294, 480.4066467285156], [295, 480.3968200683594], [296, 480.38653564453125], [297, 480.3760681152344], [298, 480.36627197265625], [299, 480.35601806640625], [300, 480.345947265625], [301, 480.3357849121094], [302, 480.3255920410156], [303, 480.3157653808594], [304, 480.30535888671875], [305, 480.29510498046875], [306, 480.28509521484375], [307, 480.2750549316406], [308, 480.2645568847656], [309, 480.254638671875], [310, 480.2447204589844], [311, 480.2345275878906], [312, 480.22454833984375], [313, 480.2145080566406], [314, 480.2044372558594], [315, 480.1942443847656], [316, 480.1844787597656], [317, 480.1741638183594], [318, 480.16436767578125], [319, 480.1541442871094], [320, 480.14410400390625], [321, 480.13427734375], [322, 480.12420654296875], [323, 480.11395263671875], [324, 480.1040954589844], [325, 480.0940856933594], [326, 480.08428955078125], [327, 480.07421875], [328, 480.0641174316406], [329, 480.0541076660156], [330, 480.044189453125], [331, 480.03436279296875], [332, 480.0245666503906], [333, 480.0146484375], [334, 480.00457763671875], [335, 479.994873046875], [336, 479.98486328125], [337, 479.97454833984375], [338, 479.96484375], [339, 479.9551696777344], [340, 479.94561767578125], [341, 479.93548583984375], [342, 479.9253845214844], [343, 479.9156188964844], [344, 479.90594482421875], [345, 479.8959045410156], [346, 479.88604736328125], [347, 479.8761291503906], [348, 479.866455078125], [349, 479.85662841796875], [350, 479.84686279296875], [351, 479.837158203125], [352, 479.82745361328125], [353, 479.8173828125], [354, 479.8076171875], [355, 479.7979431152344], [356, 479.7881774902344], [357, 479.7784118652344], [358, 479.7686462402344], [359, 479.7586975097656], [360, 479.7489318847656], [361, 479.7391662597656], [362, 479.7294921875], [363, 479.71990966796875], [364, 479.710205078125], [365, 479.70050048828125], [366, 479.6907653808594], [367, 479.6812438964844], [368, 479.6712341308594], [369, 479.6617126464844], [370, 479.65216064453125], [371, 479.642333984375], [372, 479.6328125], [373, 479.62335205078125], [374, 479.61358642578125], [375, 479.60369873046875], [376, 479.59423828125], [377, 479.5848693847656], [378, 479.57489013671875], [379, 479.5654296875], [380, 479.5557556152344], [381, 479.5462341308594], [382, 479.53643798828125], [383, 479.52667236328125], [384, 479.5174865722656], [385, 479.50787353515625], [386, 479.498291015625], [387, 479.48846435546875], [388, 479.4793395996094], [389, 479.46978759765625], [390, 479.4600524902344], [391, 479.4503479003906], [392, 479.4407653808594], [393, 479.4317932128906], [394, 479.421875], [395, 479.412353515625], [396, 479.4029235839844], [397, 479.39361572265625], [398, 479.3836975097656], [399, 479.37451171875], [400, 479.36517333984375], [401, 479.35546875], [402, 479.3462829589844], [403, 479.33660888671875], [404, 479.3270568847656], [405, 479.3175354003906], [406, 479.30828857421875], [407, 479.2989196777344], [408, 479.28936767578125], [409, 479.280029296875], [410, 479.2705993652344], [411, 479.2613830566406], [412, 479.25164794921875], [413, 479.24267578125], [414, 479.23345947265625], [415, 479.22357177734375], [416, 479.21453857421875], [417, 479.204833984375], [418, 479.19586181640625], [419, 479.1861267089844], [420, 479.17730712890625], [421, 479.1676330566406], [422, 479.15814208984375], [423, 479.14923095703125], [424, 479.1397399902344], [425, 479.13031005859375], [426, 479.12078857421875], [427, 479.1117248535156], [428, 479.102294921875], [429, 479.09320068359375], [430, 479.0841369628906], [431, 479.0746154785156], [432, 479.0654296875], [433, 479.0559997558594], [434, 479.04693603515625], [435, 479.0376892089844], [436, 479.02874755859375], [437, 479.01922607421875], [438, 479.0101013183594], [439, 479.0006408691406], [440, 478.99163818359375], [441, 478.98236083984375], [442, 478.9732360839844], [443, 478.96392822265625], [444, 478.95477294921875], [445, 478.9457092285156], [446, 478.9364318847656], [447, 478.92742919921875], [448, 478.9183044433594], [449, 478.9091796875], [450, 478.89984130859375], [451, 478.8904724121094], [452, 478.8816833496094], [453, 478.87255859375], [454, 478.863525390625], [455, 478.85430908203125], [456, 478.8451232910156], [457, 478.8360290527344], [458, 478.82684326171875], [459, 478.8177795410156], [460, 478.80865478515625], [461, 478.7996520996094], [462, 478.79071044921875], [463, 478.78173828125], [464, 478.77252197265625], [465, 478.76336669921875], [466, 478.754638671875], [467, 478.74542236328125], [468, 478.7364807128906], [469, 478.727294921875], [470, 478.718505859375], [471, 478.70947265625], [472, 478.70037841796875], [473, 478.69134521484375], [474, 478.68231201171875], [475, 478.67315673828125], [476, 478.66424560546875], [477, 478.65521240234375], [478, 478.64617919921875], [479, 478.637451171875], [480, 478.6283264160156], [481, 478.61956787109375], [482, 478.61065673828125], [483, 478.60174560546875], [484, 478.5926818847656], [485, 478.583740234375], [486, 478.57470703125], [487, 478.56585693359375], [488, 478.55682373046875], [489, 478.54815673828125], [490, 478.5392150878906], [491, 478.5301208496094], [492, 478.5216369628906], [493, 478.51287841796875], [494, 478.50372314453125], [495, 478.4945373535156], [496, 478.48602294921875], [497, 478.47735595703125], [498, 478.4683532714844], [499, 478.4593811035156]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = torch.tensor([5.0, 7.0, 12.0, 16.0, 20.0])\n",
    "y = torch.tensor([40.0, 120.0, 180.0, 210.0, 240.0])\n",
    "learning_rate = torch.tensor(0.001)\n",
    "model = RegressionModel(x,y)\n",
    "model.train(learning_rate,500)\n",
    "# print(model.total_loss)\n",
    "print(model.total_loss)\n",
    "plt.plot(range(1,501),model.total_loss,label = \"loss\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss vs. Epochs')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/200], Loss: 492.5326\n",
      "Epoch [20/200], Loss: 480.6849\n",
      "Epoch [30/200], Loss: 480.5797\n",
      "Epoch [40/200], Loss: 480.4766\n",
      "Epoch [50/200], Loss: 480.3747\n",
      "Epoch [60/200], Loss: 480.2734\n",
      "Epoch [70/200], Loss: 480.1724\n",
      "Epoch [80/200], Loss: 480.0722\n",
      "Epoch [90/200], Loss: 479.9730\n",
      "Epoch [100/200], Loss: 479.8745\n",
      "Epoch [110/200], Loss: 479.7765\n",
      "Epoch [120/200], Loss: 479.6792\n",
      "Epoch [130/200], Loss: 479.5829\n",
      "Epoch [140/200], Loss: 479.4867\n",
      "Epoch [150/200], Loss: 479.3917\n",
      "Epoch [160/200], Loss: 479.2969\n",
      "Epoch [170/200], Loss: 479.2032\n",
      "Epoch [180/200], Loss: 479.1099\n",
      "Epoch [190/200], Loss: 479.0176\n",
      "Epoch [200/200], Loss: 478.9257\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w = nn.Parameter(torch.randn(1))\n",
    "        self.b = nn.Parameter(torch.randn(1))\n",
    "    def forward(self,x):\n",
    "        return x*self.w+self.b\n",
    "\n",
    "x = torch.tensor([5.0, 7.0, 12.0, 16.0, 20.0]).view(-1,1)\n",
    "y = torch.tensor([40.0, 120.0, 180.0, 210.0, 240.0]).view(-1,1)\n",
    "lr = 0.001\n",
    "model = RegressionModel()\n",
    "loss_fuc  = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "total_loss = []\n",
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "    outputs = model(x)\n",
    "    loss = loss_fuc(outputs,y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    total_loss.append(loss.item())\n",
    "    # Print every 10th epoch\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/200], Loss: 494.7270\n",
      "Epoch [20/200], Loss: 482.7798\n",
      "Epoch [30/200], Loss: 482.6595\n",
      "Epoch [40/200], Loss: 482.5422\n",
      "Epoch [50/200], Loss: 482.4258\n",
      "Epoch [60/200], Loss: 482.3099\n",
      "Epoch [70/200], Loss: 482.1947\n",
      "Epoch [80/200], Loss: 482.0807\n",
      "Epoch [90/200], Loss: 481.9676\n",
      "Epoch [100/200], Loss: 481.8547\n",
      "Epoch [110/200], Loss: 481.7429\n",
      "Epoch [120/200], Loss: 481.6323\n",
      "Epoch [130/200], Loss: 481.5219\n",
      "Epoch [140/200], Loss: 481.4124\n",
      "Epoch [150/200], Loss: 481.3039\n",
      "Epoch [160/200], Loss: 481.1960\n",
      "Epoch [170/200], Loss: 481.0886\n",
      "Epoch [180/200], Loss: 480.9824\n",
      "Epoch [190/200], Loss: 480.8768\n",
      "Epoch [200/200], Loss: 480.7716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5612/506743333.py:36: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  plt.legend()\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1,1)\n",
    "    def forward(self,x):\n",
    "        return self.linear(x)\n",
    "\n",
    "x = torch.tensor([5.0, 7.0, 12.0, 16.0, 20.0]).view(-1,1)\n",
    "y = torch.tensor([40.0, 120.0, 180.0, 210.0, 240.0]).view(-1,1)\n",
    "lr = 0.001\n",
    "model = RegressionModel()\n",
    "loss_fuc  = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "total_loss = []\n",
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "    outputs = model(x)\n",
    "    loss = loss_fuc(outputs,y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    total_loss.append(loss.item())\n",
    "    # Print every 10th epoch\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "plt.plot(range(1,epochs+1),total_loss)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title('Loss vs. Epochs')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 4.],\n",
      "        [5., 6.],\n",
      "        [2., 8.],\n",
      "        [5., 7.],\n",
      "        [3., 1.]])\n",
      "Epoch [10/2000], Loss: 22.6313\n",
      "Epoch [20/2000], Loss: 22.2541\n",
      "Epoch [30/2000], Loss: 21.9383\n",
      "Epoch [40/2000], Loss: 21.6555\n",
      "Epoch [50/2000], Loss: 21.3992\n",
      "Epoch [60/2000], Loss: 21.1665\n",
      "Epoch [70/2000], Loss: 20.9552\n",
      "Epoch [80/2000], Loss: 20.7632\n",
      "Epoch [90/2000], Loss: 20.5887\n",
      "Epoch [100/2000], Loss: 20.4300\n",
      "Epoch [110/2000], Loss: 20.2857\n",
      "Epoch [120/2000], Loss: 20.1545\n",
      "Epoch [130/2000], Loss: 20.0350\n",
      "Epoch [140/2000], Loss: 19.9262\n",
      "Epoch [150/2000], Loss: 19.8271\n",
      "Epoch [160/2000], Loss: 19.7368\n",
      "Epoch [170/2000], Loss: 19.6545\n",
      "Epoch [180/2000], Loss: 19.5793\n",
      "Epoch [190/2000], Loss: 19.5107\n",
      "Epoch [200/2000], Loss: 19.4480\n",
      "Epoch [210/2000], Loss: 19.3907\n",
      "Epoch [220/2000], Loss: 19.3383\n",
      "Epoch [230/2000], Loss: 19.2902\n",
      "Epoch [240/2000], Loss: 19.2462\n",
      "Epoch [250/2000], Loss: 19.2058\n",
      "Epoch [260/2000], Loss: 19.1687\n",
      "Epoch [270/2000], Loss: 19.1345\n",
      "Epoch [280/2000], Loss: 19.1031\n",
      "Epoch [290/2000], Loss: 19.0740\n",
      "Epoch [300/2000], Loss: 19.0473\n",
      "Epoch [310/2000], Loss: 19.0225\n",
      "Epoch [320/2000], Loss: 18.9995\n",
      "Epoch [330/2000], Loss: 18.9782\n",
      "Epoch [340/2000], Loss: 18.9584\n",
      "Epoch [350/2000], Loss: 18.9400\n",
      "Epoch [360/2000], Loss: 18.9228\n",
      "Epoch [370/2000], Loss: 18.9067\n",
      "Epoch [380/2000], Loss: 18.8916\n",
      "Epoch [390/2000], Loss: 18.8775\n",
      "Epoch [400/2000], Loss: 18.8642\n",
      "Epoch [410/2000], Loss: 18.8516\n",
      "Epoch [420/2000], Loss: 18.8398\n",
      "Epoch [430/2000], Loss: 18.8285\n",
      "Epoch [440/2000], Loss: 18.8179\n",
      "Epoch [450/2000], Loss: 18.8077\n",
      "Epoch [460/2000], Loss: 18.7981\n",
      "Epoch [470/2000], Loss: 18.7888\n",
      "Epoch [480/2000], Loss: 18.7800\n",
      "Epoch [490/2000], Loss: 18.7715\n",
      "Epoch [500/2000], Loss: 18.7633\n",
      "Epoch [510/2000], Loss: 18.7554\n",
      "Epoch [520/2000], Loss: 18.7478\n",
      "Epoch [530/2000], Loss: 18.7404\n",
      "Epoch [540/2000], Loss: 18.7332\n",
      "Epoch [550/2000], Loss: 18.7263\n",
      "Epoch [560/2000], Loss: 18.7195\n",
      "Epoch [570/2000], Loss: 18.7129\n",
      "Epoch [580/2000], Loss: 18.7065\n",
      "Epoch [590/2000], Loss: 18.7002\n",
      "Epoch [600/2000], Loss: 18.6940\n",
      "Epoch [610/2000], Loss: 18.6879\n",
      "Epoch [620/2000], Loss: 18.6820\n",
      "Epoch [630/2000], Loss: 18.6761\n",
      "Epoch [640/2000], Loss: 18.6704\n",
      "Epoch [650/2000], Loss: 18.6647\n",
      "Epoch [660/2000], Loss: 18.6591\n",
      "Epoch [670/2000], Loss: 18.6536\n",
      "Epoch [680/2000], Loss: 18.6482\n",
      "Epoch [690/2000], Loss: 18.6428\n",
      "Epoch [700/2000], Loss: 18.6375\n",
      "Epoch [710/2000], Loss: 18.6322\n",
      "Epoch [720/2000], Loss: 18.6269\n",
      "Epoch [730/2000], Loss: 18.6218\n",
      "Epoch [740/2000], Loss: 18.6166\n",
      "Epoch [750/2000], Loss: 18.6115\n",
      "Epoch [760/2000], Loss: 18.6065\n",
      "Epoch [770/2000], Loss: 18.6015\n",
      "Epoch [780/2000], Loss: 18.5965\n",
      "Epoch [790/2000], Loss: 18.5915\n",
      "Epoch [800/2000], Loss: 18.5866\n",
      "Epoch [810/2000], Loss: 18.5817\n",
      "Epoch [820/2000], Loss: 18.5768\n",
      "Epoch [830/2000], Loss: 18.5720\n",
      "Epoch [840/2000], Loss: 18.5672\n",
      "Epoch [850/2000], Loss: 18.5624\n",
      "Epoch [860/2000], Loss: 18.5576\n",
      "Epoch [870/2000], Loss: 18.5528\n",
      "Epoch [880/2000], Loss: 18.5481\n",
      "Epoch [890/2000], Loss: 18.5434\n",
      "Epoch [900/2000], Loss: 18.5387\n",
      "Epoch [910/2000], Loss: 18.5341\n",
      "Epoch [920/2000], Loss: 18.5294\n",
      "Epoch [930/2000], Loss: 18.5248\n",
      "Epoch [940/2000], Loss: 18.5202\n",
      "Epoch [950/2000], Loss: 18.5156\n",
      "Epoch [960/2000], Loss: 18.5110\n",
      "Epoch [970/2000], Loss: 18.5064\n",
      "Epoch [980/2000], Loss: 18.5019\n",
      "Epoch [990/2000], Loss: 18.4974\n",
      "Epoch [1000/2000], Loss: 18.4929\n",
      "Epoch [1010/2000], Loss: 18.4884\n",
      "Epoch [1020/2000], Loss: 18.4839\n",
      "Epoch [1030/2000], Loss: 18.4794\n",
      "Epoch [1040/2000], Loss: 18.4750\n",
      "Epoch [1050/2000], Loss: 18.4705\n",
      "Epoch [1060/2000], Loss: 18.4661\n",
      "Epoch [1070/2000], Loss: 18.4617\n",
      "Epoch [1080/2000], Loss: 18.4573\n",
      "Epoch [1090/2000], Loss: 18.4529\n",
      "Epoch [1100/2000], Loss: 18.4486\n",
      "Epoch [1110/2000], Loss: 18.4442\n",
      "Epoch [1120/2000], Loss: 18.4399\n",
      "Epoch [1130/2000], Loss: 18.4356\n",
      "Epoch [1140/2000], Loss: 18.4313\n",
      "Epoch [1150/2000], Loss: 18.4270\n",
      "Epoch [1160/2000], Loss: 18.4227\n",
      "Epoch [1170/2000], Loss: 18.4184\n",
      "Epoch [1180/2000], Loss: 18.4142\n",
      "Epoch [1190/2000], Loss: 18.4099\n",
      "Epoch [1200/2000], Loss: 18.4057\n",
      "Epoch [1210/2000], Loss: 18.4015\n",
      "Epoch [1220/2000], Loss: 18.3973\n",
      "Epoch [1230/2000], Loss: 18.3931\n",
      "Epoch [1240/2000], Loss: 18.3889\n",
      "Epoch [1250/2000], Loss: 18.3847\n",
      "Epoch [1260/2000], Loss: 18.3806\n",
      "Epoch [1270/2000], Loss: 18.3765\n",
      "Epoch [1280/2000], Loss: 18.3723\n",
      "Epoch [1290/2000], Loss: 18.3682\n",
      "Epoch [1300/2000], Loss: 18.3641\n",
      "Epoch [1310/2000], Loss: 18.3600\n",
      "Epoch [1320/2000], Loss: 18.3560\n",
      "Epoch [1330/2000], Loss: 18.3519\n",
      "Epoch [1340/2000], Loss: 18.3479\n",
      "Epoch [1350/2000], Loss: 18.3438\n",
      "Epoch [1360/2000], Loss: 18.3398\n",
      "Epoch [1370/2000], Loss: 18.3358\n",
      "Epoch [1380/2000], Loss: 18.3318\n",
      "Epoch [1390/2000], Loss: 18.3278\n",
      "Epoch [1400/2000], Loss: 18.3238\n",
      "Epoch [1410/2000], Loss: 18.3199\n",
      "Epoch [1420/2000], Loss: 18.3159\n",
      "Epoch [1430/2000], Loss: 18.3120\n",
      "Epoch [1440/2000], Loss: 18.3080\n",
      "Epoch [1450/2000], Loss: 18.3041\n",
      "Epoch [1460/2000], Loss: 18.3002\n",
      "Epoch [1470/2000], Loss: 18.2963\n",
      "Epoch [1480/2000], Loss: 18.2925\n",
      "Epoch [1490/2000], Loss: 18.2886\n",
      "Epoch [1500/2000], Loss: 18.2847\n",
      "Epoch [1510/2000], Loss: 18.2809\n",
      "Epoch [1520/2000], Loss: 18.2771\n",
      "Epoch [1530/2000], Loss: 18.2732\n",
      "Epoch [1540/2000], Loss: 18.2694\n",
      "Epoch [1550/2000], Loss: 18.2656\n",
      "Epoch [1560/2000], Loss: 18.2618\n",
      "Epoch [1570/2000], Loss: 18.2581\n",
      "Epoch [1580/2000], Loss: 18.2543\n",
      "Epoch [1590/2000], Loss: 18.2506\n",
      "Epoch [1600/2000], Loss: 18.2468\n",
      "Epoch [1610/2000], Loss: 18.2431\n",
      "Epoch [1620/2000], Loss: 18.2394\n",
      "Epoch [1630/2000], Loss: 18.2357\n",
      "Epoch [1640/2000], Loss: 18.2320\n",
      "Epoch [1650/2000], Loss: 18.2283\n",
      "Epoch [1660/2000], Loss: 18.2246\n",
      "Epoch [1670/2000], Loss: 18.2210\n",
      "Epoch [1680/2000], Loss: 18.2173\n",
      "Epoch [1690/2000], Loss: 18.2137\n",
      "Epoch [1700/2000], Loss: 18.2100\n",
      "Epoch [1710/2000], Loss: 18.2064\n",
      "Epoch [1720/2000], Loss: 18.2028\n",
      "Epoch [1730/2000], Loss: 18.1992\n",
      "Epoch [1740/2000], Loss: 18.1956\n",
      "Epoch [1750/2000], Loss: 18.1921\n",
      "Epoch [1760/2000], Loss: 18.1885\n",
      "Epoch [1770/2000], Loss: 18.1850\n",
      "Epoch [1780/2000], Loss: 18.1814\n",
      "Epoch [1790/2000], Loss: 18.1779\n",
      "Epoch [1800/2000], Loss: 18.1744\n",
      "Epoch [1810/2000], Loss: 18.1709\n",
      "Epoch [1820/2000], Loss: 18.1674\n",
      "Epoch [1830/2000], Loss: 18.1639\n",
      "Epoch [1840/2000], Loss: 18.1604\n",
      "Epoch [1850/2000], Loss: 18.1569\n",
      "Epoch [1860/2000], Loss: 18.1535\n",
      "Epoch [1870/2000], Loss: 18.1500\n",
      "Epoch [1880/2000], Loss: 18.1466\n",
      "Epoch [1890/2000], Loss: 18.1432\n",
      "Epoch [1900/2000], Loss: 18.1398\n",
      "Epoch [1910/2000], Loss: 18.1364\n",
      "Epoch [1920/2000], Loss: 18.1330\n",
      "Epoch [1930/2000], Loss: 18.1296\n",
      "Epoch [1940/2000], Loss: 18.1262\n",
      "Epoch [1950/2000], Loss: 18.1229\n",
      "Epoch [1960/2000], Loss: 18.1195\n",
      "Epoch [1970/2000], Loss: 18.1162\n",
      "Epoch [1980/2000], Loss: 18.1129\n",
      "Epoch [1990/2000], Loss: 18.1095\n",
      "Epoch [2000/2000], Loss: 18.1062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12433/1671879050.py:39: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  plt.legend()\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(2,1)\n",
    "    def forward(self,x):\n",
    "        return self.linear(x)\n",
    "\n",
    "x = torch.tensor([[3.0, 4.0, 5.0, 6.0, 2.0],[8,5,7,3,1]])\n",
    "x=torch.reshape(x,[5,2])\n",
    "print(x)\n",
    "y = torch.tensor([-3.7,3.5,2.5,11.5,5.7]).view(-1,1)\n",
    "lr = 0.001\n",
    "model = RegressionModel()\n",
    "loss_fuc  = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "total_loss = []\n",
    "epochs = 2000\n",
    "for epoch in range(epochs):\n",
    "    outputs = model(x)\n",
    "    loss = loss_fuc(outputs,y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    total_loss.append(loss.item())\n",
    "    # Print every 10th epoch\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "plt.plot(range(1,epochs+1),total_loss)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title('Loss vs. Epochs')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid(x))\n\u001b[0;32m----> 9\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m70\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m75\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m()\n\u001b[1;32m     10\u001b[0m y \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     11\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(x)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'float'"
     ]
    }
   ],
   "source": [
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1,1)\n",
    "    def sigmoid(self,x):\n",
    "        return 1/(1+torch.exp(-x))\n",
    "    def forward(self,x):\n",
    "        return self.linear(self.sigmoid(x))\n",
    "x = [1, 5, 10, 10, 25, 50, 70, 75, 100]\n",
    "y = [0, 0, 0, 0, 0, 1, 1, 1, 1]\n",
    "x = torch.tensor(x).view(-1,1).float()\n",
    "y = torch.tensor(y).view(-1,1).float()\n",
    "lr = 0.001\n",
    "model = RegressionModel()\n",
    "loss_fuc  = nn.BCEWithLogitsLoss\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "total_loss = []\n",
    "epochs = 2000\n",
    "for epoch in range(epochs):\n",
    "    outputs = model(x)\n",
    "    loss = loss_fuc(outputs,y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    total_loss.append(loss.item())\n",
    "    # Print every 10th epoch\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "plt.plot(range(1,epochs+1),total_loss)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title('Loss vs. Epochs')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
